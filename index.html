<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance">
  <meta name="keywords" content="Reward Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TimeRewarder</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance</h1>
          <!-- <h3 class="title is-4 conference-name"><a target="_blank" href="https://iclr.cc/Conferences/2024">ICLR 2024</a></h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yuyang Liu<sup>1, 2, *</sup>,</span>
            <span class="author-block">
              Chuan Wen<sup>3, *</sup>,</span>
            <span class="author-block">
              Yihang Hu<sup>1, 2</sup>,</span>
            <span class="author-block">
              Dinesh Jayaraman<sup>4</sup>,</span>
            <span class="author-block">
              Yang Gao<sup>1, 2</sup>
            </span>
          </div>
        </br>
          <div class="is-size-6 text_gray">
            <span class="author-block"><sup>1</sup>Institute for Interdisciplinary Information Sciences, Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Qi Zhi Institute,</span>
            <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>4</sup>University of Pennsylvania</span>
          </div>

          <div class="is-size-6 text_gray"> *Equal Contribution </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Openreview Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="https://openreview.net/forum?id=BAwXV3Y0cc&noteId=BAwXV3Y0cc"
                   class="external-link button is-danger is-rounded is-light">
                  <span class="icon">
                      <i class="fas fa-file"></i>
                  </span>
                  <span>OpenReview</span>
                </a>
              </span> -->
              <!-- arXiv Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2509.26627"
                   class="external-link button is-success is-rounded is-light">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-info is-rounded is-light">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <video id="example-result-video"
                  muted
                  autoplay
                  loop
                  width="100%">
            <source src="img/basketball_new.mp4"
                    type="video/mp4">
          </video>
        <!-- </br> -->
        <!-- </br> -->
          <p>
            Designing effective rewards has long been a bottleneck for reinforcement learning in robotics—
            dense rewards often require extensive manual engineering and do not scale.
            Our work introduces <strong>TimeRewarder</strong>, a simple yet powerful method that learns
            reward signals directly from passive videos, including robot demonstrations and even human activity.
            By modeling how far apart two video frames are in task progress,
            TimeRewarder provides step-wise proxy rewards that make reinforcement learning dramatically more efficient.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">From Observation to Reward</span></h2>
        </br>
        <img src="img/concept.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
          
        <p>
          Humans can often judge how close a task is to completion simply by watching it unfold,
          without knowing the exact actions being taken. 
          Inspired by this intuition, TimeRewarder learns to infer task progress 
          directly from videos of expert demonstrations. 
        </p>
        <p>
          At its core, TimeRewarder models the <em>temporal distance</em> between two frames: 
          which one is closer to task completion, and by how much. 
          These predictions translate naturally into dense, step-wise rewards, 
          allowing reinforcement learning agents to evaluate whether they are making progress 
          at every step. 
        </p>
        <p>
          By distilling progress signals from passive videos without action labels or manual supervision,
          TimeRewarder provides a scalable path toward guiding robots through complex tasks 
          using nothing more than visual observation.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">TimeRewarder: Training and Reward Generation</span></h2>
        </br>
        <img src="img/method.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
          
        <p>
          TimeRewarder is trained on expert videos in a self-supervised way to predict 
          <em>temporal distances</em> between frames. 
          The model learns which ordered pair of states represenst progress and which corresponds
          to regressions or suboptimal behaviors.
        </p>

        <p>
          During reinforcement learning, TimeRewarder converts these predicted distances between consecutive 
          observations into dense rewards. Positive distances indicate meaningful advancement toward task completion, 
          while negative distances penalize unproductive actions.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">Reinforcement Learning Performance</span></h2>
          

  </br>
        <img src="img/tasks_visualization.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
  </br>
  <p>
    On 9 out of 10 Meta-World tasks, TimeRewarder achieved the highest final success rates and sample efficiency, 
    even surpassing policies trained with the environment’s ground-truth dense reward (green dashed line). 
    This demonstrates that video-based, progress-driven rewards can replace manually designed 
    reward functions while delivering superior performance.
  </p>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">


  <h2 class="title is-3"><span class="text_blue">Reward Quality</span></h2>
  </br>
        <img src="img/vocvis.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
  </br>
  <p> 
    Across all tasks, TimeRewarder consistently produced the highest Value–Order Correlation(VOC)
    scores on held-out expert videos, demonstrating strong temporal coherence and the ability to generalize beyond 
    training data.
  </p>

  </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">

  </br>
        <img src="img/rewardvis.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
  </br>

  <p>
    Unlike prior methods, TimeRewarder clearly distinguished meaningful progress from suboptimal behavior, 
    assigning lower or negative rewards when the agent stalled or regressed. This temporally 
    coherent feedback makes it easier for RL agents to recover from mistakes.
  </p>

  </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">

  <h2 class="title is-3"><span class="text_blue">Adding Cross-Domain Videos to Boost Performance</span></h2>
  </br>
        <img src="img/humancross.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
  </br>
  <p>
    TimeRewarder can leverage cross-domain videos to further improve learning. 
    For tasks with only a single in-domain demonstration, supplementing with 20 human videos 
    dramatically increased success rates. This shows that the method can utilize unlabeled, 
    diverse visual data to scale reward learning across different domains and embodiments.
  </p>
      </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="rows">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue"></span></h2>
        <p>
          To address the above issue, we propose encouraging the agent to master earlier parts of demonstrated behaviors before proceeding to subsequent ones. 
          Our method incorporates a dynamic scheduling mechanism for the discount factor: start with a relatively small discount factor to prioritize the imitation of early episode behaviors; 
          as the agent advances in the task, the discount factor increases adaptively, allowing the agent to tackle later stages only after it has effectively learned the earlier behaviors.
        </p>
      <div class="row is-four-fifths">
        <div class="row is-centered">
          <img src="markdown_version/index.assets/ADS.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </div>
      </div>
    </div>
  </div>


<section class="section">
  <div class="container is-max-desktop">
    <div class="rows">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">Experiment Results</span></h2>
        <p>
          Comparison against start-of-the-art ILfO baselines on 9 challenging Meta-World tasks, with 8 random seeds.
        </p>
    </br>
        <img src="markdown_version/index.assets/result.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
      </br>
         <p>
          Here is the visualization of the discount factor scheduled during the training process of different tasks.
        </p>
      </br>
        <img src="markdown_version/index.assets/adaptive.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
      </div>
    </div>
  </div> -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title"><span class="text_blue">Citation</span></h2>
    <pre><code><span class="text_gray">@inproceedings{
liu2024imitation,
title={Imitation Learning from Observation with Automatic Discount Scheduling},
author={Yuyang Liu and Weijun Dong and Yingdong Hu and Chuan Wen and Zhao-Heng Yin and Chongjie Zhang and Yang Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pPJTQYOpNI}
}
    </span></code></pre>
  </div>
</section> -->
