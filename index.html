<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance">
  <meta name="keywords" content="Reward Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TimeRewarder</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance</h1>
          <!-- <h3 class="title is-4 conference-name"><a target="_blank" href="https://iclr.cc/Conferences/2024">ICLR 2024</a></h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yuyang Liu<sup>1, 2, *</sup>,</span>
            <span class="author-block">
              Chuan Wen<sup>3, *</sup>,</span>
            <span class="author-block">
              Yihang Hu<sup>1, 2</sup>,</span>
            <span class="author-block">
              Dinesh Jayaraman<sup>4</sup>,</span>
            <span class="author-block">
              Yang Gao<sup>1, 2</sup>
            </span>
          </div>
        </br>
          <div class="is-size-6 text_gray">
            <span class="author-block"><sup>1</sup>Institute for Interdisciplinary Information Sciences, Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Qi Zhi Institute,</span>
            <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>4</sup>University of Pennsylvania,</span>
          </div>

          <div class="is-size-6 text_gray"> *Equal Contribution </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Openreview Link. -->
              <span class="link-block">
                <a target="_blank" href="https://openreview.net/forum?id=BAwXV3Y0cc&noteId=BAwXV3Y0cc"
                   class="external-link button is-danger is-rounded is-light">
                  <span class="icon">
                      <i class="fas fa-file"></i>
                  </span>
                  <span>OpenReview</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-success is-rounded is-light">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-info is-rounded is-light">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <!-- <video id="example-result-video"
                  muted
                  autoplay
                  loop
                  width="100%">
            <source src="markdown_version/index.assets/example.mp4"
                    type="video/mp4">
          </video> -->
        </br>
        </br>
          <p>
            Designing effective rewards has long been a bottleneck for reinforcement learning in robotics—
            dense rewards often require extensive manual engineering and do not scale.
            Our work introduces <strong>TimeRewarder</strong>, a simple yet powerful method that learns
            reward signals directly from passive videos, including robot demonstrations and even human activity.
            By modeling how far apart two video frames are in task progress,
            TimeRewarder provides step-wise proxy rewards that make reinforcement learning dramatically more efficient.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">When and Why Previous Proxy-Reward-Based Methods Fail?</span></h2>
        <p>
          Consider the following <i class="text_blue">basketball</i> task. If we (a) only instruct the agent 
          to learn the expert's early behaviors (reaching for and grasping the ball), the agent quickly acquires these skills. 
          However, when (b) tasked with learning the entire expert demonstration, the same method fails to acquire the initial 
          grasping skill, and instead, moves the empty gripper directly to the basket. This is an intriguing finding: 
          <i class="text_blue">rewarding later steps in a trajectory seems to negatively impact the agent’s ability to learn the earlier behaviors.</i>
        </p>
        </br>
        <img src="markdown_version/index.assets/motivation.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        
          <p>
            We observe a similar phenomenon in many manipulation tasks characterized by <b  class="text_green">progress dependencies</b>. 
            In these tasks, the agents trained by previous proxy-reward-based approaches often fail to mimic the 
            expert's early behaviors, and resort to optimizing rewards in later stages by moving to states 
            that appear similar to demonstrated states. These locally optimal but incorrect solutions can hinder 
            the agent's exploration of earlier critical behaviors.
          </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="rows">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">Solution: Automatic Discount Scheduling (ADS)</span></h2>
        <p>
          To address the above issue, we propose encouraging the agent to master earlier parts of demonstrated behaviors before proceeding to subsequent ones. 
          Our method incorporates a dynamic scheduling mechanism for the discount factor: start with a relatively small discount factor to prioritize the imitation of early episode behaviors; 
          as the agent advances in the task, the discount factor increases adaptively, allowing the agent to tackle later stages only after it has effectively learned the earlier behaviors.
        </p>
      <div class="row is-four-fifths">
        <div class="row is-centered">
          <img src="markdown_version/index.assets/ADS.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </div>
      </div>
    </div>
  </div>


<section class="section">
  <div class="container is-max-desktop">
    <div class="rows">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="text_blue">Experiment Results</span></h2>
        <p>
          Comparison against start-of-the-art ILfO baselines on 9 challenging Meta-World tasks, with 8 random seeds.
        </p>
    </br>
        <img src="markdown_version/index.assets/result.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
      </br>
         <p>
          Here is the visualization of the discount factor scheduled during the training process of different tasks.
        </p>
      </br>
        <img src="markdown_version/index.assets/adaptive.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
      </div>
    </div>
  </div>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title"><span class="text_blue">Citation</span></h2>
    <pre><code><span class="text_gray">@inproceedings{
liu2024imitation,
title={Imitation Learning from Observation with Automatic Discount Scheduling},
author={Yuyang Liu and Weijun Dong and Yingdong Hu and Chuan Wen and Zhao-Heng Yin and Chongjie Zhang and Yang Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pPJTQYOpNI}
}
    </span></code></pre>
  </div>
</section>
